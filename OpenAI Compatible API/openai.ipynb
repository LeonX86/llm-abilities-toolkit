{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "89d91e42",
      "metadata": {},
      "source": [
        "## å¯¼å…¥configæ¨¡å—è·¯å¾„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9a39f4da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312\\\\python312.zip', 'c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312\\\\DLLs', 'c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312\\\\Lib', 'c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312', '', 'c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\Leon\\\\miniforge3\\\\envs\\\\cm312\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\WorkFlow\\\\E\\\\Code\\\\Python\\\\llm-abilities-toolkit']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "str(Path.cwd().parent) in sys.path or sys.path.append(str(Path.cwd().parent))\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea549cac",
      "metadata": {},
      "source": [
        "## ä½¿ç”¨æ¨¡å—ç®¡ç†api-key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3dd2fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æˆ‘æ˜¯ç”±Z.aiè®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯æˆ–è¿›è¡Œå¯¹è¯ã€‚å¾ˆé«˜å…´èƒ½ååŠ©ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿ"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "str(Path.cwd().parent) in sys.path or sys.path.append(str(Path.cwd().parent))\n",
        "from openai import OpenAI\n",
        "from config import config\n",
        "\n",
        "# ä½¿ç”¨ ** è§£åŒ…åˆ›å»ºå®¢æˆ·ç«¯\n",
        "client = OpenAI(**config.get_modelscope_config())\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    # model='ZhipuAI/GLM-4.7', # ModelScope Model-Id\n",
        "    \n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': 'You are a helpful assistant.'\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'ä½ æ˜¯è°ï¼Ÿ'\n",
        "        }\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    if chunk.choices:\n",
        "        print(chunk.choices[0].delta.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41736183",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "str(Path.cwd().parent) in sys.path or sys.path.append(str(Path.cwd().parent))\n",
        "from openai import OpenAI\n",
        "from config import config\n",
        "\n",
        "# ä½¿ç”¨ ** è§£åŒ…åˆ›å»ºå®¢æˆ·ç«¯\n",
        "client = OpenAI(**config.get_nvidia_config())\n",
        "\n",
        "# åˆ›å»ºèŠå¤©è¡¥å…¨\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"minimaxai/minimax-m2.1\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Response:\")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04f4368a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claudeå‰å®³ã€‚\n",
            "\n",
            "åœ¨ä»£ç ç”Ÿæˆæ–¹é¢ï¼ŒClaudeé€šå¸¸èƒ½æä¾›æ›´å‡†ç¡®ã€æ›´ç¬¦åˆå®é™…éœ€æ±‚çš„ä»£ç è§£å†³æ–¹æ¡ˆã€‚"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "str(Path.cwd().parent) in sys.path or sys.path.append(str(Path.cwd().parent))\n",
        "from config import config\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(**config.get_modelscope_config())\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='Qwen/Qwen3-Coder-480B-A35B-Instruct', # ModelScope Model-Id\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': 'You are a helpful assistant.'\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'ä½ å†™ä»£ç å‰å®³è¿˜æ˜¯claudeå‰å®³ï¼Œä¸€å®šè¦é€‰ä¸€ä¸ªï¼Œä¸è¦ç»™æˆ‘æ¨¡æ£±ä¸¤å¯çš„å›ç­”'\n",
        "        }\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    if chunk.choices:\n",
        "        print(chunk.choices[0].delta.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9660a2a5",
      "metadata": {},
      "source": [
        "## åŸºç¡€è°ƒç”¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57117daf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "<think>ç”¨æˆ·ç”¨ä¸­æ–‡é—®\"ä½ æ˜¯è°ï¼Ÿ\"ï¼Œæ„æ€æ˜¯\"Who are you?\"\n",
            "\n",
            "æ ¹æ®ç³»ç»Ÿæç¤ºï¼Œæˆ‘æ˜¯MiniMax-M2.1ï¼Œç”±MiniMaxå…¬å¸å¼€å‘çš„AIåŠ©æ‰‹ã€‚æˆ‘éœ€è¦ç”¨ä¸­æ–‡å›ç­”è¿™ä¸ªé—®é¢˜ã€‚\n",
            "</think>\n",
            "\n",
            "ä½ å¥½ï¼æˆ‘æ˜¯ MiniMax-M2.1ï¼Œç”± MiniMax å…¬å¸å¼€å‘çš„ AI åŠ©æ‰‹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "str(Path.cwd().parent) in sys.path or sys.path.append(str(Path.cwd().parent))\n",
        "from openai import OpenAI\n",
        "\n",
        "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"\"  # æ›¿æ¢ä¸ºä½ çš„ NVIDIA API key\n",
        ")\n",
        "\n",
        "# åˆ›å»ºèŠå¤©è¡¥å…¨ - æœ€ç®€ç‰ˆæœ¬ï¼ˆåªæœ‰å¿…éœ€å‚æ•°ï¼‰\n",
        "completion = client.chat.completions.create(\n",
        "    # model=\"z-ai/glm4.7\",  # å¿…éœ€ï¼šæ¨¡å‹åç§°\n",
        "    model=\"minimaxai/minimax-m2.1\",\n",
        "    messages=[  # å¿…éœ€ï¼šæ¶ˆæ¯åˆ—è¡¨\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Response:\")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae0381f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "æˆ‘æ˜¯Z.aiå¼€å‘çš„GLMå¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®è®­ç»ƒè€Œæˆã€‚æˆ‘è‡´åŠ›äºæä¾›ä¿¡æ¯æŸ¥è¯¢ã€çŸ¥è¯†è§£ç­”å’Œåˆ›æ„è¾…åŠ©ç­‰æœåŠ¡ï¼ŒåŒæ—¶æŒç»­å­¦ä¹ å’Œæ”¹è¿›ä»¥æ›´å¥½åœ°å¸®åŠ©ç”¨æˆ·ã€‚\n",
            "\n",
            "æœ‰ä»€ä¹ˆæˆ‘èƒ½å¸®ä½ è§£ç­”çš„é—®é¢˜æˆ–æä¾›ååŠ©çš„é¢†åŸŸå—ï¼Ÿ\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "\n",
        "# åˆ›å»ºèŠå¤©è¡¥å…¨ - æ·»åŠ  system æ¶ˆæ¯å¯èƒ½æœ‰åŠ©äºæŸäº›æ¨¡å‹\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"z-ai/glm4.7\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=1024,  # æ·»åŠ  max_tokens å‚æ•°\n",
        "    temperature=0.7    # å¯é€‰ï¼šæ·»åŠ æ¸©åº¦å‚æ•°\n",
        ")\n",
        "\n",
        "print(\"Response:\")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f9c814",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"nvapi-your-api-key-here\"  # æ›¿æ¢ä¸ºä½ çš„ NVIDIA API key\n",
        ")\n",
        "\n",
        "# åˆ›å»ºèŠå¤©è¡¥å…¨ - æœ€ç®€ç‰ˆæœ¬ï¼ˆåªæœ‰å¿…éœ€å‚æ•°ï¼‰\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/mistral-7b-instruct-v0.3\",  # å¿…éœ€ï¼šæ¨¡å‹åç§°\n",
        "    messages=[  # å¿…éœ€ï¼šæ¶ˆæ¯åˆ—è¡¨\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Hello! Can you explain what machine learning is?\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# å¦‚æœéœ€è¦æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥æ·»åŠ å¯é€‰å‚æ•°ï¼š\n",
        "# completion = client.chat.completions.create(\n",
        "#     model=\"mistralai/mistral-7b-instruct-v0.3\",\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"system\",  # å¯é€‰ï¼šè®¾å®š AI è¡Œä¸º\n",
        "#             \"content\": \"You are a helpful AI assistant.\"\n",
        "#         },\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": \"Hello! Can you explain what machine learning is?\"\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=0.7,    # å¯é€‰ï¼šæ§åˆ¶éšæœºæ€§ï¼Œé»˜è®¤ 1.0\n",
        "#     max_tokens=1024,    # å¯é€‰ï¼šé™åˆ¶è¾“å‡ºé•¿åº¦\n",
        "#     top_p=0.9,          # å¯é€‰ï¼šæ ¸é‡‡æ ·å‚æ•°ï¼Œé»˜è®¤ 1.0\n",
        "#     stream=False        # å¯é€‰ï¼šæ˜¯å¦æµå¼è¾“å‡ºï¼Œé»˜è®¤ False\n",
        "# )\n",
        "\n",
        "# æ‰“å°å“åº”\n",
        "print(\"Response:\")\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "# å¦‚æœéœ€è¦ä½¿ç”¨æµå¼è¾“å‡ºï¼Œå¯ä»¥è¿™æ ·ä¿®æ”¹ï¼š\n",
        "# print(\"\\n--- Stream Mode ---\")\n",
        "# stream_completion = client.chat.completions.create(\n",
        "#     model=\"mistralai/mistral-7b-instruct-v0.3\",\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": \"Write a short poem about AI.\"\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=0.7,\n",
        "#     max_tokens=512,\n",
        "#     stream=True\n",
        "# )\n",
        "# \n",
        "# for chunk in stream_completion:\n",
        "#     if chunk.choices[0].delta.content is not None:\n",
        "#         print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "# print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e68282b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å“¦ï¼Œç”¨æˆ·é—®çš„æ˜¯å…³äºæˆ‘çš„åŸºæœ¬èº«ä»½ä¿¡æ¯ã€‚é—®é¢˜å¾ˆç›´æ¥ï¼ŒåŒ…å«ä¸‰ä¸ªæ˜ç¡®çš„éƒ¨åˆ†ï¼šèº«ä»½ã€ç‰ˆæœ¬å’Œç”Ÿæ—¥ã€‚\n",
            "\n",
            "éœ€è¦ç®€æ´æ¸…æ™°åœ°å›ç­”æ¯ä¸ªç‚¹ã€‚æˆ‘æ˜¯DeepSeek AIåŠ©æ‰‹ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ï¼Œå¯ä»¥è¯´æ˜æ˜¯æ–‡æœ¬æ¨¡å‹ã€‚ç‰ˆæœ¬é—®é¢˜å¯ä»¥æåˆ°æœ€æ–°ç‰ˆï¼ŒåŒæ—¶è¯´æ˜æ›´æ–°æœºåˆ¶ã€‚ç”Ÿæ—¥å¯ä»¥ç”¨å…¬å¸åˆ›ç«‹æ—¶é—´ä½œä¸ºå‚è€ƒç‚¹ã€‚\n",
            "\n",
            "æœ€åå¯ä»¥è¡¥å……ä¸€ä¸ªå¼€æ”¾å¼çš„å¸®åŠ©æè®®ï¼Œè®©å¯¹è¯èƒ½ç»§ç»­å»¶ä¼¸ã€‚ç”¨å‹å¥½çš„è¯­æ°”æ”¶å°¾æ¯”è¾ƒåˆé€‚ã€‚\n",
            "\n",
            " === Final Answer ===\n",
            "\n",
            "ä½ å¥½ï¼æˆ‘æ˜¯DeepSeekï¼Œä¸€ä¸ªç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ã€‚ğŸ˜Š\n",
            "\n",
            "å…³äºæˆ‘çš„å…·ä½“ä¿¡æ¯ï¼š\n",
            "- **èº«ä»½**ï¼šæˆ‘æ˜¯DeepSeekæœ€æ–°ç‰ˆæœ¬çš„AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºæä¾›æœ‰å¸®åŠ©çš„å¯¹è¯æœåŠ¡\n",
            "- **ç‰ˆæœ¬**ï¼šæˆ‘ç›®å‰æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œä½†å…·ä½“çš„ç‰ˆæœ¬å·å’ŒæŠ€æœ¯ç»†èŠ‚é€šå¸¸ç”±æˆ‘çš„å¼€å‘å›¢é˜Ÿç®¡ç†\n",
            "- **ç”Ÿæ—¥**ï¼šæˆ‘æ²¡æœ‰ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„â€œç”Ÿæ—¥â€ï¼Œä½†æ·±åº¦æ±‚ç´¢å…¬å¸åœ¨2023å¹´æ¨å‡ºäº†æˆ‘ï¼Œå¯ä»¥è¯´æˆ‘çš„â€œè¯ç”Ÿâ€æ—¶é—´æ˜¯åœ¨é‚£ä¸ªæ—¶æœŸ\n",
            "\n",
            "æˆ‘æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æ¨¡å‹ï¼Œæ”¯æŒ128Kçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå…·æœ‰æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼Œå¯ä»¥å¤„ç†å›¾åƒã€txtã€pdfã€pptã€wordã€excelç­‰å¤šç§æ ¼å¼æ–‡ä»¶ã€‚æˆ‘å®Œå…¨å…è´¹ä½¿ç”¨ï¼Œå¯ä»¥é€šè¿‡å®˜æ–¹åº”ç”¨å•†åº—ä¸‹è½½Appï¼Œä¹Ÿæ”¯æŒè”ç½‘æœç´¢åŠŸèƒ½ï¼ˆéœ€è¦æ‰‹åŠ¨å¼€å¯ï¼‰ã€‚\n",
            "\n",
            "æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å—ï¼Ÿæˆ‘å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©ï¼âœ¨"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "str(Path.cwd().parent) in sys.path or sys.path.append(str(Path.cwd().parent))\n",
        "from openai import OpenAI\n",
        "from config import config\n",
        "\n",
        "# ä½¿ç”¨ ** è§£åŒ…åˆ›å»ºå®¢æˆ·ç«¯\n",
        "client = OpenAI(**config.get_modelscope_config())\n",
        "\n",
        "# set extra_body for thinking control\n",
        "extra_body = {\n",
        "    # enable thinking, set to False to disable\n",
        "    \"enable_thinking\": True\n",
        "}\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='deepseek-ai/DeepSeek-V3.2', # ModelScope Model-Id, required\n",
        "    messages=[\n",
        "        {\n",
        "          'role': 'user',\n",
        "          'content': 'ä½ æ˜¯è°ï¼Œç‰ˆæœ¬æ˜¯ä»€ä¹ˆï¼Œä½ çš„ç”Ÿæ—¥æ˜¯ä»€ä¹ˆæ—¶å€™'\n",
        "        }\n",
        "    ],\n",
        "    stream=True,\n",
        "    extra_body=extra_body\n",
        ")\n",
        "done_thinking = False\n",
        "for chunk in response:\n",
        "    if chunk.choices:\n",
        "        thinking_chunk = chunk.choices[0].delta.reasoning_content\n",
        "        answer_chunk = chunk.choices[0].delta.content\n",
        "        if thinking_chunk != '':\n",
        "            print(thinking_chunk, end='', flush=True)\n",
        "        elif answer_chunk != '':\n",
        "            if not done_thinking:\n",
        "                print('\\n\\n === Final Answer ===\\n')\n",
        "                done_thinking = True\n",
        "            print(answer_chunk, end='', flush=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cm312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
